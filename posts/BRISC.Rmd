---
title: "Reading Notes on BRISC: Bootstrap for Rapid Inference on Spatial Covariances"
date: 2025-02-28T19:22:00Z
draft: false
description: "This post explores the BRISC method for spatial covariance estimation."
categories:
  - papers
  - spatial statistics
tags:
  - BRISC
  - bootstrap
  - spatial covariance
  - Gaussian process
output: md_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to Gaussian Processes

A Gaussian process (GP) is a collection of random variables, any finite number of which have Gaussian distributions. A Gaussian process is fully specified by a mean function $\mu(x)$ and covariance function $K(x,x')$:
$$f(x) \sim \mathcal{GP} (m(x), K(x,x'))$$


Gaussian Processes (GPs) are widely used in machine learning, statistics, and spatial modeling due to their unique properties and flexibility. Here are some key reasons for their popularity:

1. **Analytical Tractability**:
   - GPs provide a **closed-form solution** for many problems, making them analytically tractable.
   - For example, the posterior distribution of a GP can be derived explicitly, allowing for exact inference in many cases.

2. **Marginal and Conditional Distributions**:
   - Any **marginal distribution** of a GP is also Gaussian. This means that if you take a subset of the random variables in a GP, their joint distribution remains Gaussian.
   - Similarly, the **conditional distribution** of a GP is Gaussian. This property is particularly useful for making predictions at new locations, as the conditional distribution can be computed analytically.

3. **Flexibility in Modeling**:
   - GPs can model complex, **non-linear relationships** by choosing an appropriate covariance (kernel) function.
   - Common kernel functions include the **Radial Basis Function (RBF)**, **Matérn**, and **Exponential kernels**, each of which captures different types of relationships in the data.

4. **Probabilistic Predictions**:
   - GPs provide **uncertainty estimates** along with predictions. This is crucial for decision-making in applications like Bayesian optimization, where understanding the uncertainty is as important as the prediction itself.

5. **Applications**:
   - GPs are widely used in **geostatistics** (e.g., kriging), **machine learning** (e.g., regression, classification), and **Bayesian optimization** (e.g., hyperparameter tuning).
   - They are also used in **time series analysis**, **robotics**, and **environmental modeling**.

6. **Kernel Design**:
   - The choice of kernel function allows GPs to capture a wide range of behaviors, such as periodicity, smoothness, and trends.
   - Kernels can also be combined or adapted to create more complex models.

7. **Interpretability**:
   - The parameters of the kernel function often have intuitive interpretations, such as length scales or variance, making GPs more interpretable than some other machine learning models.


Despite their many advantages, GPs face a significant computational bottleneck: the need to invert the covariance matrix $( K(x,x') $, which has $ O(n^3) $ complexity, where n is the number of observations). For large datasets, this becomes infeasible, limiting the scalability of traditional GPs.

To address the computational challenges of traditional GPs, Nearest Neighbor Gaussian Process (NNGP) has been developed. NNGP approximates the full GP by limiting dependencies between data points to a small subset of nearest neighbors. This reduces the computational complexity while retaining the key properties of GPs, making it a scalable alternative for large datasets.

## Nearest neighbor Gaussian process(NNGP)

Nearest Neighbor Gaussian Process (NNGP) is a powerful method for approximating Gaussian Processes (GPs) to make them computationally feasible for large datasets. Traditional GPs require inverting a dense covariance matrix, which has a computational complexity of $ O(n^3) $  and becomes infeasible for large n. NNGP addresses this challenge by approximating the full conditional distribution using only a subset of nearest neighbors.

The intuition behind NNGP is that spatial correlation decays with distance. In other words, data points that are far apart have little influence on each other. Therefore, instead of considering all data points, NNGP assumes that each data point depends only on a small subset of its nearest neighbors. This reduces the computational complexity to $ O(nm^2) $,where 
m is the number of neighbors, making NNGP scalable for large datasets.

## Introduction to Bootstrap

The **bootstrap** is a resampling technique used to estimate the distribution of a statistic by repeatedly sampling with replacement from the observed data. It is widely used for:

- Estimating confidence intervals.
- Assessing the variability of statistical estimates.
- Performing hypothesis testing.

The bootstrap is computationally efficient because it avoids the need for complex analytical derivations or assumptions about the underlying distribution.

## Computational Advantage over MCMC

- **MCMC (Markov Chain Monte Carlo)**: MCMC methods are computationally intensive, requiring many iterations to converge to the target distribution. They are often used for Bayesian inference but can be slow for large datasets or complex models.
- **Bootstrap**: In contrast, the bootstrap is non-parametric and does not require sampling from a posterior distribution. It is often faster and easier to implement, making it a practical alternative for many applications.

## Challenges in Spatial Data

While the bootstrap is widely applicable, its **independence assumption** is often violated in spatial data. In spatial datasets:

- Observations are typically **correlated** due to spatial proximity (e.g., temperature measurements at nearby locations are likely similar).
- Ignoring this correlation can lead to biased or inaccurate estimates.

## Decorrelating Spatial Data with Cholesky Decomposition

To apply the bootstrap to spatial data, we need to account for spatial correlation. One common approach is to use **Cholesky decomposition** to decorrelate the data before resampling. Here’s how it works:

1. **Covariance Matrix**: Compute the covariance matrix $ \Sigma $ of the spatial data, which captures the correlation structure.
2. **Cholesky Decomposition**: Factorize $ \Sigma $ into $ \Sigma = LL^T $, where $ L $ is a lower triangular matrix.
3. **Decorrelate the Data**: Transform the original data $ Y $ into decorrelated data $ Z $ using:
   $$
   Z = L^{-1} Y
   $$
   This transformation removes the spatial correlation, making the data approximately independent.
4. **Resample**: Apply the bootstrap to the decorrelated data $ Z $.
5. **Recorrelate**: Transform the resampled data back to the original space using:
   $$
   Y^* = L Z^*
  $$
   where $Z^*$ is the resampled decorrelated data and $ Y^* $ is the resampled spatial data.

### Advantages of Decorrelated Bootstrap

- **Preserves Spatial Structure**: By decorrelating and recorrelating the data, the bootstrap respects the spatial correlation structure.
- **Computationally Efficient**: Cholesky decomposition is relatively fast compared to MCMC, making this approach scalable for large datasets.
- **Flexible**: This method can be applied to a wide range of spatial models and datasets.

## Example Application

In spatial statistics, the decorrelated bootstrap is often used to:

- Estimate confidence intervals for spatial predictions (e.g., kriging).
- Assess the uncertainty of parameter estimates in spatial models.
- Validate the performance of spatial models.

## Mathematical Details

For those interested in the mathematical formulation, here’s a brief overview:

### Covariance Matrix

The covariance matrix $ \Sigma $ is defined as:
$$
\Sigma_{ij} = k(x_i, x_j)
$$
where $ k(x_i, x_j) $ is the spatial covariance function (e.g., RBF kernel).

### Cholesky Decomposition

The Cholesky decomposition of $ \Sigma $ is:
$$
\Sigma = LL^T
$$
where $ L $ is a lower triangular matrix.

### Decorrelating the Data

The decorrelated data $ Z $ is computed as:
$$
Z = L^{-1} Y
$$
This transformation ensures that $ Z $ has approximately independent components.

### Resampling and Recorrelating

After resampling $ Z $ to obtain $ Z^* $, the resampled spatial data $ Y^* $ is computed as:
$$
Y^* = L Z^*
$$


## BRISC: Boostrap for rapid inference on spatial covariances

While applying the bootstrap to spatial data can decrease computational time compared to traditional methods like MCMC, it is still not optimal for very large datasets due to the need to decorrelate the data, which still involves expensive Cholesky decomposition of the covariance matrix. The **BRISC (Bootstrap for Rapid Inference on Spatial Covariances)** method addresses this challenge by combining the **bootstrap** with the **Nearest Neighbor Gaussian Process (NNGP)**.

 
Mathematically, using the chain rule of probability, the joint likelihood can be wrote as  
$$
p(y) = p(y(x_1)) \prod_{i=2}^n p(y(x_i) \mid y(x_1), y(x_2), \dots, y(x_{i-1}))
$$

By approximation where each term depends only on a neighborhood $ \mathcal{N}(x_i) $:

$$
\tilde{p}(y) = p(y(x_1)) \prod_{i=2}^n p(y(x_i) \mid y(\mathcal{N}(x_i)))
$$

Here, $ \mathcal{N}(x_i) $ represents the set of variables that $ y(x_i) $ depends on.

This approximation corresponds to a **Nearest-Neighbor Gaussian Process (NNGP)** model, which is a computationally efficient approximation of a full Gaussian Process (GP). The NNGP model is defined as:

$$
y \sim \mathcal{N}(X\beta, \Sigma_e),
$$

where the precision matrix $\Sigma_e^{-1}$ admits the factorization:

$$
\Sigma_e^{-1} = (I - A)^\top D (I - A). \quad (3)
$$

Here:
- $A$ is a **sparse lower triangular matrix** with at most $m$ non-zero entries in each row. These non-zero entries correspond to the nearest neighbors of each location.
- $D = \text{diag}(d_{ii})$ is a **diagonal matrix**.

The sparsity of $A$ (controlled by $m$) makes the NNGP model computationally efficient, as it reduces the complexity of matrix operations. Typically, a small value of $m$ (e.g., 10 or 20) is sufficient to achieve results that are effectively indistinguishable from the full GP model.

In BRISC algorithm, the Cholesky factor $\Sigma^{1/2}$ in the decorrelation step can be replaced with its surrogate $\Sigma_e^{1/2}$, which is derived from the factorization of the precision matrix $\Sigma_e^{-1}$ in the NNGP model. This surrogate is computationally efficient due to the sparsity of $A$ and the diagonal nature of $D$.

From the factorization in (3):

$$
\Sigma_e^{-1} = (I - A)^\top D (I - A),
$$

the surrogate Cholesky factor $\Sigma_e^{1/2}$ is given by:

$$
\Sigma_e^{1/2} = D^{1/2} (I - A).
$$

Here:
- $D^{1/2}$ is the square root of the diagonal matrix $D$.
- $I - A$ is a sparse lower triangular matrix with at most $m$ non-zero entries per row.

---

### Decorrelation Step

The decorrelation step transforms the original data $y$ into a decorrelated vector $\tilde{y}$:

$$
\tilde{y} = \Sigma_e^{1/2} (y - X\beta).
$$

Because $D$ is diagonal and $I - A$ is sparse, this computation requires only $O(mn)$ floating-point operations (FLOPs), where $n$ is the number of observations and $m$ is the number of non-zero entries per row in $A$.

---

### Distribution of $\tilde{y}$

Under the true model, each entry of the decorrelated vector $\tilde{y}$ will be **identically distributed**. This is because:

1. The transformation $\tilde{y} = \Sigma_e^{1/2} (y - X\beta)$ removes the spatial dependence structure encoded in $\Sigma_e$.
2. The resulting vector $\tilde{y}$ consists of independent and identically distributed (i.i.d.) random variables, typically following a standard normal distribution $\mathcal{N}(0, I)$.

Since $\tilde{y}$ is mostly uncorrelated, the bootstrap can then be applied.


### Advantages of BRISC

- **Computational Efficiency**: By combining NNGP and bootstrap, BRISC achieves significant computational savings compared to traditional methods like MCMC.
- **Scalability**: BRISC is highly scalable and can handle large spatial datasets that are infeasible with traditional GPs.


## Reference

* Saha, A., & Datta, A. (2018). BRISC: bootstrap for rapid inference on spatial covariances. Stat, 7(1), e184.
